general:
  destination:
    server: https://kubernetes.default.svc
  # This is the namespace where the Application resources will live, typically the same where Argo CD is running.
  namespace: argocd
  nginx:
    namespace: nginx-ingress
    source:
      chart: nginx-ingress
      repoURL: https://kubernetes-charts.storage.googleapis.com/
      targetRevision: 1.36.2

argo:
  enabled: false
  # If deploying into kubernetes.default.svc, this namespace should be different than general.namespace.
  namespace: argocd-local
  source:
    chart: argo-cd
    repoURL: https://argoproj.github.io/argo-helm
    targetRevision: 2.2.2
  finalizer:
    enabled: false
  spec:
    ingress:
      enabled: true
      annotations:
        nginx.ingress.kubernetes.io/configuration-snippet: |
          proxy_set_header l5d-dst-override $service_name.$namespace.svc.cluster.local:$service_port;
          grpc_set_header l5d-dst-override $service_name.$namespace.svc.cluster.local:$service_port;
        kubernetes.io/ingress.class: "nginx-private"
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
      hosts:
      - argocd-sre.gke.some-domain.com
      tls:
      - secretName: argocd-tls
        hosts:
        - argocd-sre.gke.some-domain.com
    installCRDs: false

certManager:
  enabled: true
  # Do not install cert-manager in a namespace that's not called "cert-manager" without first adjusting the CRD YAML.
  namespace: cert-manager
  source:
    chart: cert-manager
    repoURL: https://charts.jetstack.io
    targetRevision: v0.15.0-alpha.2
  finalizer:
    enabled: false
  spec:
    global:
      leaderElection:
        namespace: cert-manager
      rbac:
        create: true
    nodeSelector: {}
    tolerations: []
    webhook:
      nodeSelector: {}
      tolerations: []
    cainjector:
      nodeSelector: {}
      tolerations: []

externalDNS:
  enabled: true
  namespace: external-dns
  source:
    chart: external-dns
    repoURL: https://charts.bitnami.com/bitnami
    targetRevision: 2.20.13
  finalizer:
    enabled: false
  spec:
    domainFilters:
    - coda.run
    provider: aws
    rbac:
      create: true
    updateStatus: true
    nodeSelector: {}
    tolerations: []

nginx_ingress_private:
  enabled: false
  finalizer:
    enabled: false
  spec:
    controller:
      autoscaling:
        enabled: true
        minReplicas: 2
        targetCPUUtilizationPercentage: 60
        targetMemoryUtilizationPercentage: 60
      extraArgs:
        enable-ssl-passthrough: true
      ingressClass: nginx-private
      publishService:
        enabled: true
      service:
        annotations:
          cloud.google.com/load-balancer-type: Internal
      podAnnotations:
        config.linkerd.io/proxy-cpu-limit: "1"
        config.linkerd.io/proxy-cpu-request: "0.5"
        config.linkerd.io/proxy-memory-limit: 1Gi
        config.linkerd.io/proxy-memory-request: 256Mi
        linkerd.io/inject: enabled
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: "1"
          memory: 512Mi
      nodeSelector:
        cloud.google.com/gke-nodepool: infra-components
      tolerations:
        - effect: NoSchedule
          key: infra
          operator: Equal
          value: "true"
    defaultBackend:
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 128Mi
      nodeSelector:
        cloud.google.com/gke-nodepool: infra-components
      tolerations:
        - effect: NoSchedule
          key: infra
          operator: Equal
          value: "true"

nginx_ingress_public:
  enabled: true
  finalizer:
    enabled: false
  spec:
    controller:
      autoscaling:
        enabled: true
        minReplicas: 2
        targetCPUUtilizationPercentage: 60
        targetMemoryUtilizationPercentage: 60
      extraArgs:
        enable-ssl-passthrough: true
      ingressClass: nginx-public
      metrics:
        enabled: true
      # serviceMonitor is for use with Prometheus Operator (requires additional CRD)
      # serviceMonitor:
      #   enabled: true
      publishService:
        enabled: true
      service:
        annotations:
          # cloud.google.com/load-balancer-type: Public
          service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
          service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
          service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'
      podAnnotations: []
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: "1"
          memory: 512Mi
      nodeSelector: {}
      tolerations: []
    defaultBackend:
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 128Mi
      nodeSelector: {}
      tolerations: []

vault:
  enabled: true
  namespace: vault
  source:
    chart: vault
    repoURL: https://mkorejo.github.io/helm_charts
    targetRevision: 0.5.0
  finalizer:
    enabled: false
  spec:
    injector:
      enabled: false
    server:
      standalone:
        enabled: false
      ha:
        enabled: true
        config: |
          ui = true

          listener "tcp" {
            tls_disable = 1
            address = "[::]:8200"
            cluster_address = "[::]:8201"
          }

          storage "consul" {
            path = "vault"
            address = "HOST_IP:8500"
          }

          service_registration "kubernetes" {}

          # seal "awskms" {
          #   region     = "us-east-1"
          #   kms_key_id = "d7c1ffd9-8cce-45e7-be4a-bb38dd205966"
          # }

          # seal "gcpckms" {
          #    project     = "vault-helm-dev-246514"
          #    region      = "global"
          #    key_ring    = "vault-helm-unseal-kr"
          #    crypto_key  = "vault-helm-unseal-key"
          # }
      ingress:
        enabled: true
        annotations: |
          kubernetes.io/ingress.class: "nginx-public"
          cert-manager.io/cluster-issuer: "letsencrypt-prod"
        hosts:
          - host: vault-sandbox.coda.run
        tls:
          - secretName: vault-tls
            hosts:
              - vault-sandbox.coda.run
    ui:
      enabled: true
    nameOverride: ""
    fullnameOverride: ""
    resources: {}
    nodeSelector: {}
    tolerations: []

velero:
  enabled: true
  namespace: velero
  source:
    path: charts/velero
    repoURL: https://github.com/vmware-tanzu/helm-charts
    targetRevision: velero-2.8.1
  finalizer:
    enabled: false
  spec:
    image:
      repository: velero/velero
      tag: v1.2.0
      pullPolicy: IfNotPresent
    resources: {}
    initContainers:
      - name: velero-plugin-for-gcp
        image: velero/velero-plugin-for-gcp:v1.0.0
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - mountPath: /target
            name: plugins
    nodeSelector: {}
    tolerations: []
    metrics:
      enabled: true
      scrapeInterval: 30s
      # Pod annotations for Prometheus
      podAnnotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8085"
        prometheus.io/path: "/metrics"
      serviceMonitor:
        enabled: false
        additionalLabels: {}
    # Parameters for the `default` BackupStorageLocation and VolumeSnapshotLocation, and additional server settings.
    configuration:
      provider: gcp
      # Parameters for the `default` BackupStorageLocation - https://velero.io/docs/v1.0.0/api-types/backupstoragelocation/
      backupStorageLocation:
        name: gcp
        bucket: sre-001-velero
        prefix: infra-apps-test
        config: {}
      # https://velero.io/docs/v1.0.0/api-types/volumesnapshotlocation/
      volumeSnapshotLocation:
        name: gcp
        config: {}
    rbac:
      create: true
      clusterAdministrator: true
    serviceAccount:
      server:
        create: true
        name:
        annotations:
    credentials:
      useSecret: true
      existingSecret: gcp-velero-sa-key
    snapshotsEnabled: true